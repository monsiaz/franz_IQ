<p>La latence ressentie dans une PWA de test de QI n’est pas un détail technique: c’est un facteur de mesure. Si l’interface tarde de 150 ms avant de refléter un tap, l’utilisateur perçoit une inertie, modifie son rythme de réponse, et votre estimation devient moins précise. En tant que psychométricien obsédé par la clarté visuelle et la rigueur, je conçois chaque écran pour que le délai entre intention et feedback soit proche de zéro, tout en restant accessible et honnête. Voici une méthode concrète pour diagnostiquer, réduire et neutraliser l’impact de la latence ressentie, sans sacrifier validité, fidélité et éthique.</p>

<h2>Pourquoi la latence ressentie altère la validité d’un test visuel</h2>
<p><strong>La validité passe par la constance des conditions de passation</strong>. Un test visuel moderne (matrices, rotations mentales, séries logiques) suppose que le temps de présentation des stimuli et la réactivité de l’interface ne varient pas de manière significative d’un item à l’autre ou d’un appareil à l’autre.</p>

<p>Les repères humains sont connus depuis longtemps: environ 100 ms est perçu comme instantané, 200 à 300 ms commence à être ressenti, 1 seconde casse le flux (heuristiques de Nielsen). En milieu web, Google Web Vitals formalise ces seuils via INP (Input Latency), CLS (stabilité) et LCP (temps de rendu principal). En psychophysique, la variabilité de latence d’affichage et de saisie (jitter) augmente la variance des temps de réaction indépendamment de la capacité cognitive (travaux rapportés par Plant et Turner; comparaisons récentes sur jsPsych et Gorilla par Anwyl‑Irvine et collègues).</p>

<p><strong>Ce que cela implique psychométriquement</strong>:</p>

<p>1) Les items avec contrainte de temps stricte deviennent sensibles au modèle d’appareil, au mode d’économie d’énergie, au thread bloqué.</p>

<p>2) Les paramètres d’item (difficulté b, discrimination a en IRT) peuvent se déplacer si le rythme interactionnel est ralenti ou saccadé.</p>

<p>3) La fidélité intra‑individu chute: la moitié de la variance de temps peut être logistique, pas cognitive, si on ne contrôle pas la latence.</p>

<p>4) Équité: un public avec appareils d’entrée de gamme ou réseaux saturés est pénalisé. C’est contraire aux principes ISO 9241 et aux attentes des évaluations à fort enjeu.</p>

<p>En bref, <strong>la latence ressentie n’est pas seulement UX</strong>: c’est un bias de mesure. Il faut donc la traiter comme une variable expérimentale, instrumentée et neutralisée.</p>

<h2>Mesurer ce qui compte dans une PWA: du clic au pixel</h2>
<p>On ne peut améliorer que ce que l’on observe correctement. Les métriques génériques ne suffisent pas. Je distingue la latence perçue globale et la latence critique pour l’item.</p>

<p><strong>Métriques générales utiles</strong> (références: Web Vitals 2024, W3C WebPerf):</p>

<p>• INP: latence d’interaction sur l’ensemble de la session, utile pour le ressenti global.</p>

<p>• LCP: temps de rendu du contenu principal, surtout pour l’écran d’accueil et les amorces de section.</p>

<p>• CLS: stabilité visuelle; un décalage en plein item peut détourner l’attention.</p>

<p>• Long Tasks API: détecte les blocs de >50 ms sur le thread principal.</p>

<p><strong>Métriques spécifiques au test</strong>:</p>

<p>• SOA (Stimulus Onset Asynchrony) cible: délai entre intention d’afficher l’item et premier pixel effectivement peint. Mesuré par performance.now() + requestAnimationFrame, plutôt que setTimeout, pour coller au cycle de rafraîchissement.</p>

<p>• ITPL (Input‑To‑Paint Latency): temps entre l’événement PointerDown/KeyDown et la première frame reflétant l’état choisi (mise en surbrillance, verrouillage du choix, timer figé). C’est la latence ressentie côté réponse.</p>

<p>• Jitter d’onset inter‑items: variance du délai d’apparition sur une batterie de 30 items. Nous visons un écart‑type < 8 ms sur appareils récents, < 20 ms sur l’entrée de gamme.</p>

<p>• Drift du chronomètre: écart entre le temps cible d’un item chronométré et le temps réel affiché (ex. 20 s demandées, 20,3 s réellement visibles). Les tests à contrainte de temps doivent corriger ce drift dans les logs.</p>

<p><strong>Instrumentation minimale</strong> (sources: W3C High Resolution Time, Event Timing, MDN Service Workers):</p>

<p>• Utiliser performance.now() pour chaque marqueur critique (préparation des stimuli, attach des listeners, première frame de l’item, première frame de feedback).</p>

<p>• Envelopper les handlers d’input dans measure() + rAF pour capturer l’ITPL réel.</p>

<p>• Activer Event Timing (First/Latent Input) et pointerrawupdate pour affiner sur stylet/souris.</p>

<p>• Journaliser la cadence de rafraîchissement (60/90/120 Hz) et l’état de battery saver; ils influencent l’ordonnanceur.</p>

<p>• Comptabiliser les Long Tasks pour chaque item; un pic > 100 ms pendant une réponse devrait déclencher un tag de qualité.</p>

<p>• Synchroniser audio/visuel avec WebAudio (clock monotone) si vous utilisez des bips d’alerte; le clock audio est plus stable que les timers génériques.</p>

<h2>Procédure opérationnelle de réduction de la latence ressentie</h2>
<p>Je propose une méthode concrète, reproductible, orientée test visuel. Elle inclut entrées, étapes, sorties et risques.</p>

<p><strong>Entrées</strong>:</p>

<p>• Parc d’appareils cible (gammes basse/moyenne/haute, Android/iOS, 4–6 pouces). Profil thermique et GPU connus.</p>

<p>• Budget de session (ex. 12 minutes, 36 items) et modèle de test (fixe vs adaptatif IRT).</p>

<p>• Contraintes d’accessibilité (WCAG 2.2 AA: contraste, focus, taille cible, motion reduced).</p>

<p>• Infrastructure (CDN HTTP/2/3, possibilité d’Edge Caching, Service Worker autorisé).</p>

<p><strong>Étapes</strong>:</p>

<p>1) Définir des budgets de latence alignés humainement: ITPL médian < 60 ms, 95e < 120 ms; SOA moyen < 120 ms entre items; 0 jank visible pendant les manipulations d’éléments.</p>

<p>2) Préparer les assets critiques hors thread principal: rasteriser les stimuli en OffscreenCanvas dans un Web Worker, générer les SVG statiques pendant l’écran précédent, précharger la palette et les géométries. Les matrices de type Raven, par exemple, sont composées de 9 tuiles: préparez les 9 rendus et la version surlignée de chaque option avant que l’utilisateur ne voie l’item.</p>

<p>3) Mettre en cache les items et leur logique: via Service Worker, Cache Storage, versionnés (Cache‑Bust) et précisés par manifest. Précharger N+2 items pour les tests adaptatifs (afin d’éviter le trou de latence au moment de la sélection IRT).</p>

<p>4) Orchestrater le rendu avec rAF: ne jamais déclencher un layout bloquant au moment du tap. Utiliser CSS transform/opacity pour transitions (< 150 ms) afin d’exploiter le GPU. Éviter la reflow cascade en regroupant style/measure/mutate (pattern RAF triple).</p>

<p>5) Prioriser l’input: inscrire les listeners en passive=true pour le scroll, mais pointerdown ne doit pas être passive. Détecter Input Predictors (Chrome) si disponibles pour raccourcir la chaîne d’input. Déplacer les calculs lourds (note partielle, logs compressés) en idleCallback ou Worker.</p>

<p>6) Simplifier le DOM: une surface d’item ne devrait pas excéder 120–180 nœuds; préférer un seul canvas ou SVG groupé à 30 div imbriquées. Polices variables: charger 1 fonte avec font-display:swap et restreindre les styles.</p>

<p>7) Throttling réseau: strictement aucun fetch pendant un item. Toute télémétrie part en batch toutes les 3–5 réponses ou à l’écran intermédiaire, avec retry hors ligne.</p>

<p>8) Dégradations contrôlées: si la cadence tombe sous 45 FPS deux fois de suite, basculer automatiquement vers un thème d’item “léger” (moins d’animations, textures retirées) et noter ce mode dans les logs.</p>

<p>9) Mesurer en continu et afficher peu: un petit marqueur d’état (optionnel) montre que l’input a été pris en compte instantanément, pas un spinner. Feedback haptique bref sur mobile si activé par l’utilisateur.</p>

<p>10) Rejouer les sessions en labo: sur deux appareils représentatifs, filmer à 240 FPS (ou 120 si nécessaire) pour estimer l’ITPL réel vs télémétrie.</p>

<p><strong>Sorties</strong>:</p>

<p>• Tableau de bord par modèle d’appareil: ITPL médian/p95, SOA médian/p95, nombre de Long Tasks par item, taux de fallback “léger”.</p>

<p>• Rapport de conformité aux budgets, avec recommandations itemisées (ex. “Item 14: jank lié au recalcul de grille; déplacer en Worker”).</p>

<p>• Drapeaux qualité par session pour correction statistique potentielle (voir section scoring).</p>

<p><strong>Risques et parades</strong>:</p>

<p>• Risque: illusions anti‑éthiques (spinner trompeur). Parade: feedback immédiat véridique (état sélectionné local), sans simuler une vitesse inexistante.</p>

<p>• Risque: fuite mémoire due à OffscreenCanvas non libéré. Parade: pool de buffers, finalisation à l’écran de pause.</p>

<p>• Risque: sur‑optimisation qui complexifie l’accessibilité. Parade: tests WCAG automatisés (axe contraste, focus visible) + revue manuelle à la loupe (contraste 4.5:1 minimum).</p>

<h2>Conception visuelle: clarté qui accélère, accessibilité qui stabilise</h2>
<p>La clarté visuelle universelle réduit à la fois la charge cognitive et la charge GPU. <strong>Moins de bruit visuel = moins de jank</strong>.</p>

<p><strong>Stimuli vectoriels sobres</strong>: préférer SVG/Canvas pur pour les figures abstraites (formes géométriques, matrices). Pas de textures bitmap lourdes, pas d’ombres portées métallisées. Un fond lisse, contraste de 12:1 pour les éléments interactifs, zones de clic de 44 px min (WCAG 2.2 cible 2.5.8).</p>

<p><strong>État focus explicite et rapide</strong>: anneaux de focus natifs conservés, renforcés par un halo net. Les changements d’état doivent être une seule propriété (opacity/transform), 120–150 ms, et honorent prefers-reduced-motion. Pas d’animations directionnelles inutiles qui déclenchent recalculs.</p>

<p><strong>Mise en page stable</strong>: éviter fonts tardives ou images qui poussent la grille; utiliser tailles réservées. CLS proche de 0. Pour des séries d’icônes (ex. 8 choix), fixer les dimensions avant chargement.</p>

<p><strong>Couleurs et contrastes</strong>: utiliser 3 niveaux maximum par écran (fond, stimuli, action). La couleur ne doit jamais être le seul canal d’information; motifs ou contours renforcent. Cela réduit l’ambiguïté et donc le temps de micro‑décision.</p>

<p><strong>Gestes et latence</strong>: un tap est plus rapide à confirmer qu’un glisser. Pour items de sélection, éviter drag-and-drop sur mobile; préférer tap‑to‑select avec verrouillage instantané visuel.</p>

<p><strong>Texte minimal et localisé tard</strong>: mes tests privilégient des consignes visuelles. Quand un texte est nécessaire, il est court, lisible, et préchargé. Moins de reflow texte = moins d’à‑coups.</p>

<p><strong>Exemple concret</strong>: item “trou dans la matrice” type Raven.</p>

<p>• Pré‑rendu: 9 cases en SVG + 8 options en bas, deux états par option (neutre/sélectionné) rasterisés en OffscreenCanvas.</p>

<p>• Interaction: pointerdown met l’option en surbrillance et verrouille la grille dans la même frame (ITPL visé < 50 ms). Un son facultatif de confirmation est déclenché à l’instant exact du paint via WebAudio.</p>

<p>• Accessibilité: boutons de 56 px, focus visible, alternative textuelle courte lisible par lecteur d’écran (“Option 3: motif triangle”). Aucun mouvement forcé si l’utilisateur a réduit les animations.</p>

<h2>Neutraliser la latence côté scoring: modèles et garde-fous</h2>
<p>Optimiser l’interface ne suffit pas: il faut aussi <strong>rendre le score robuste à la latence résiduelle</strong>. Transparence: un test est une estimation, jamais une vérité absolue.</p>

<p><strong>1) Calibrage de latence par session</strong>: insérer 2 items “tampons” non scorés au début, conçus pour mesurer ITPL et jitter sur l’appareil de l’utilisateur, sans leur imposer de contrainte mentale. Par exemple, un simple appui sur la forme qui s’allume aléatoirement. On enregistre un profil de latence (médiane, p95) par session.</p>

<p><strong>2) Temps comme covariable, pas comme critère brut</strong>: pour les tests non speedés, on n’intègre pas le temps de réponse au score de capacité; on surveille plutôt le pattern (ex. chutes soudaines). Pour les tests mêlant vitesse et précision, on suit des cadres publiés (modèles vitesse‑précision de van der Linden): temps log-normal comme variable latente distincte, liée mais non confondue avec la capacité.</p>

<p><strong>3) Correction d’exposition et adaptatif</strong>: dans un CAT (test adaptatif) IRT, la sélection d’item dépend des réponses. Si un item présente un jank anormal détecté (Long Task > 150 ms pendant la réponse), on peut atténuer son poids dans la mise à jour bayésienne (ex. doubler l’écart type de l’item pour cette observation) ou marquer l’observation comme moins fiable, documenté dans le rapport.</p>

<p><strong>4) Fenêtres temporelles équitables</strong>: si un item est limité à 20 s visibles, on corrige en retirant le drift mesuré (temps écran réel) plutôt que le temps planifié. Ainsi, personne n’est pénalisé par un scheduler lent.</p>

<p><strong>5) Détection d’anomalies</strong>: une séquence de délais d’interaction supérieurs à 300 ms associée à des Long Tasks signale un contexte dégradé. On peut proposer une pause technique, rappeler l’option hors ligne, ou déplacer les items les plus sensibles au temps vers la fin.</p>

<p><strong>6) Étalonnage multi‑appareils</strong>: lors de la construction des normes, on stratifie par classe d’appareil et de navigateur. Les paramètres d’item sont ancrés via un design commun (ancrage d’items) et vérifiés en DIF (Differential Item Functioning) pour s’assurer qu’un item n’est pas injustement plus difficile sur une classe donnée.</p>

<p><strong>Exemple de règle de scoring honnête</strong>: “Si la session présente un ITPL p95 > 180 ms ou plus de 2 items avec jank > 150 ms pendant la réponse, le score est accompagné d’un indicateur de qualité et d’un intervalle de confiance élargi.” C’est transparent, compréhensible, et conforme à l’éthique: on ne sur‑promet pas une précision que les conditions ne permettent pas.</p>

<h2>Tests, terrain, et suivi continu: ce que j’exige avant un déploiement</h2>
<p>Je refuse d’exposer un test à large public sans une validation technique et psychométrique conjointe. Voici ma procédure de mise en production:</p>

<p><strong>Essais synthétiques</strong>:</p>

<p>• Lighthouse/Pagespeed: viser INP vert (< 200 ms) même sous CPU x4. LCP < 2,5 s au premier écran.</p>

<p>• WebPageTest avec réseau simulé (4G moyenne, 3G forte) et CPU moyen/bas. Comparer ITPL simulé.</p>

<p>• Profils de Long Tasks sur des scénarios d’items lourds (rotations, multiples options).</p>

<p><strong>Essais sur appareils réels</strong>:</p>

<p>• Milieu de gamme Android (2 à 3 Go RAM), iPhone d’il y a 4 générations, tablette 60 Hz. Mesure caméra haute vitesse pour valider la télémétrie.</p>

<p>• Sessions de 12–15 minutes pour repérer la dérive thermique (throttling). Si ITPL augmente au fil de la session, réorganiser les items ou insérer une micro‑pause visuelle toutes les 10 réponses.</p>

<p><strong>RUM éthique en production</strong> (Real User Monitoring):</p>

<p>• Collecter les métriques spécifiques (SOA, ITPL, Long Tasks), anonymisées et agrégées, opt‑in clair et conforme RGPD. Pas de traceur intrusif.</p>

<p>• Dashboards hebdos par item et par version. Déploiement progressif (canary 5–10 %), rollback si p95 ITPL dégrade de > 30 ms.</p>

<p><strong>Garde‑fous</strong>:</p>

<p>• Mode hors ligne complet via Service Worker, incluant les 20 prochains items et la feuille de style. L’utilisateur termine même sans réseau; on synchronise plus tard.</p>

<p>• Interdiction de fetch pendant l’item, rappelée par tests automatisés (lint de réseau). Les images non essentielles sont lazy‑loadées, mais jamais pendant une interaction.</p>

<p>• Alertes automatiques si un bundle dépasse le budget (ex. 150 Ko gzip pour le runtime critique). Split par routes et par type d’item.</p>

<p>• Audit WCAG régulier avec axe sur “Reduire les mouvements” et “Focus visible” car un changement d’état de focus visible rassure et accélère le ressenti d’instantanéité.</p>

<h2>Erreurs fréquentes et comment les éviter en contexte de QI visuel</h2>
<p><strong>Erreur 1: mettre le scoring sur le thread principal</strong>. Calculer les paramètres IRT, même simplifiés, lors du tap bloque le feedback. Solution: file d’événements; le tap fige l’UI en 1 frame, et la logique part en Worker. Le prochain item n’est affiché qu’après ack du Worker, mais l’utilisateur a déjà le feedback instantané.</p>

<p><strong>Erreur 2: animations décoratives</strong>. Les micro‑animations non informatives sabotent la stabilité temporelle. Solution: transitions brèves et utiles; supprimer toute animation sur items lourds; respecter prefers-reduced-motion.</p>

<p><strong>Erreur 3: police web tardive</strong>. Un FOUT/FOIT au moment d’un item décale la grille. Solution: font‑display: swap; taille réserver; ou système de fallback robuste.</p>

<p><strong>Erreur 4: dépendre du réseau pour la logique adaptative</strong>. Un round‑trip au serveur pour choisir l’item suivant introduit du trou. Solution: embarquer un sous‑ensemble d’items calibrés localement, avec un estimateur local (EAP) qui synchronise par batch.</p>

<p><strong>Erreur 5: mesure naïve du temps</strong>. setTimeout dérive; Date.now est peu précis. Solution: performance.now + rAF pour l’affichage; WebAudio pour l’audio; toujours mesurer la frame de feedback, pas la fin du handler.</p>

<p><strong>Erreur 6: négliger la stabilité visuelle</strong>. Un CLS même faible pendant la décision rompt l’attention. Solution: dimensions figées, hiérarchie stable, pas d’insertion de bannières ou d’infos contextuelles pendant un item.</p>

<p><strong>Erreur 7: uniformiser les appareils</strong>. Penser qu’“optimisé pour le dernier iPhone” suffit. Solution: optimiser en priorité pour des appareils moyens; tester le pire scénario; documenter les limites d’appareils non supportés.</p>

<h2>Exemples pas à pas: de l’item brut à l’item fluide</h2>
<h3>Item “rotation mentale” en 3 étapes</h3>
<p>Étape 1 – Baseline naïve:</p>

<p>• Trois images PNG de 200 Ko chacune, transition en fade 300 ms, calculs d’angles au moment du tap, logs instantanés vers le serveur.</p>

<p>Résultat: ITPL p95 ~ 220 ms sur Android milieu de gamme; jank lors de la transition; drift de timer de 300 ms sur un lot de 10 items.</p>

<p>Étape 2 – Assainissement:</p>

<p>• Conversion en SVG; pré‑rendu OffscreenCanvas; transitions CSS transform 120 ms; logs batchés; suppression de la requête réseau pendant l’item; Worker pour le calcul.</p>

<p>Résultat: ITPL p95 ~ 110 ms; jank éliminé; INP vert en stress test.</p>

<p>Étape 3 – Finition psychométrique:</p>

<p>• Item tampon de calibrage latence; correction du drift par session; tag qualité si Long Task survient pendant la réponse; modèle vitesse‑précision au scoring.</p>

<p>Résultat: score plus stable inter‑sessions; intervalle de confiance réaliste pour sessions dégradées.</p>

<h3>Item “sélection de l’intrus” sans texte</h3>
<p>Avant: 8 vignettes avec ombres et hover CSS lourds; fonts tardives; tap qui déclenche simultanément analytics et navigation.</p>

<p>Après: 8 vignettes en grille figée, contours nets, surbrillance immédiate au pointerdown, analytics en idle. ITPL médian 38 ms sur iOS ancien; utilisateurs rapportent plus de fluidité et moins d’erreurs de double‑tap.</p>

<h2>Sourcing et références pour aller plus loin, sans mystifier</h2>
<p>Ce que je préconise s’appuie sur des sources publiques et robustes. Les seuils de perception proviennent des heuristiques de Jakob Nielsen et d’usages confirmés par Google’s RAIL et Web Vitals. Les APIs à utiliser sont documentées par le W3C (High Resolution Time, Long Tasks, Event Timing) et MDN (Service Workers, OffscreenCanvas). Du côté psychométrie, les modèles vitesse‑précision de van der Linden sont la base pour séparer capacité et vitesse; l’IRT moderne (2PL/3PL) nécessite une invariance des paramètres d’item qu’on ne peut garantir qu’en contrôlant la technique. Sur le timing web, plusieurs équipes ont benchmarké la précision de JavaScript et des navigateurs pour des expériences cognitives (Plant, Turner; Garaizar & Reips; Anwyl‑Irvine et al. sur jsPsych/Gorilla). Enfin, les critères WCAG 2.2 et ISO 9241 guident l’accessibilité et la charge cognitive.</p>

<p>Cette transparence n’est pas un vernis: <strong>le test est une estimation</strong>. Notre responsabilité est d’en communiquer la précision, les conditions de validité, et les zones d’incertitude, plutôt que d’afficher des chiffres clinquants.</p>

<h2>Vers des tests PWA qui transcendent l’appareil, sans trahir la mesure</h2>
<p>Nous sommes à un moment charnière. Les PWA ont gagné en capacités (Workers, Cache, WebAssembly, OffscreenCanvas), et les navigateurs deviennent plus prévisibles. Demain, WebGPU permettra des rendus plus lourds à coût constant, et Shared Storage offrira des agrégations privées pour un RUM respectueux de la vie privée. Mais les principes resteront: <strong>clarté visuelle universelle, rigueur de mesure, accessibilité par design, transparence et éthique</strong>.</p>

<p>Ma boussole est simple: si une optimisation “vend du rêve” mais fausse la perception ou la mesure, je la refuse. Si une micro‑latence subsiste malgré tout, je la mesure, je la documente, et j’en neutralise l’impact statistiquement. À ce prix, nos tests visuels sur mobile deviendront ce qu’ils doivent être: rapides à utiliser, lents à mentir, et justes, quel que soit l’appareil.</p>